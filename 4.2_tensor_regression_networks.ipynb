{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Regression Networks with ``TensorLy`` and ``gluon``\n",
    "\n",
    "Let's now see how to combine TensorLy and MXNet in to implement the tensor regression networks, as defined in **Tensor Regression Network**, _Jean Kossaifi, Zachary C. Lipton, Aran Khanna, Tommaso Furlanello and Anima Anandkumar_, [ArXiV pre-publication](https://arxiv.org/abs/1707.08308).\n",
    "\n",
    "We test the model on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using mxnet backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet.gluon import Block\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.random import check_random_state\n",
    "\n",
    "mx.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am writing this tutorial from my Eurostar seat with limited access to the internet and only my laptop's CPU at hand, so CPU it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply load the MNIST dataset as a numpy array and split the training and testing into MXNet's NDArrayIter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = mx.test_utils.get_mnist()\n",
    "batch_size = 64\n",
    "num_outputs = 10\n",
    "train_data = mx.io.NDArrayIter(mnist[\"train_data\"], mnist[\"train_label\"], batch_size, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(mnist[\"test_data\"], mnist[\"test_label\"], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Tensor Regression Layer as a gluon Block\n",
    "\n",
    "We wrap the code needed to perform low-rank tensor regression into a Gluon block so we can connect it to a deep convolutional net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRL(Block):\n",
    "    def __init__(self, ranks, n_outputs, in_shape=0, verbose=1, **kwargs):\n",
    "        super(TRL, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.ranks = tuple(ranks)\n",
    "            self.n_outputs = n_outputs\n",
    "            self.weight_initializer = mx.init.Xavier(magnitude=2.24)\n",
    "            self.core = self.params.get(\n",
    "                'core', init=self.weight_initializer, \n",
    "                 shape=self.ranks)\n",
    "            self.bias = self.params.get('bias', shape=(1, n_outputs))\n",
    "            self.factors = [self.params.get(\n",
    "                'factor_{}'.format(i), init=self.weight_initializer,\n",
    "                 allow_deferred_init=True,\n",
    "                 shape=(0, e)) for (i, e) in enumerate(ranks[:-1])]\n",
    "            self.factors.append(self.params.get(\n",
    "                'factor_{}'.format(len(ranks) - 1), init=self.weight_initializer,\n",
    "                 shape=(n_outputs, ranks[-1])))\n",
    "            self.initialised_factors = False\n",
    "            self.verbose = verbose\n",
    "            self.displayed_info = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialised_factors:\n",
    "            if self.verbose:\n",
    "                print('TRL: initializing factors') \n",
    "            for i in range(len(self.ranks) - 1):\n",
    "                self.factors[i].shape = (x.shape[i+1], self.ranks[i])\n",
    "                self.factors[i]._finish_deferred_init()\n",
    "            self.initialised_factors = True\n",
    "            if self.verbose:\n",
    "                print('  ..done.')\n",
    "        \n",
    "        if self.verbose and not self.displayed_info:\n",
    "            print('TRL on input of shape={} with batch size={}.'.format(\n",
    "                            x.shape[1:], x.shape[0], self.n_outputs))\n",
    "            print('    TRL-rank={} corresponding to ({} [inputs], {} [outputs])'.format(\n",
    "                            self.core.shape, x.shape[1:], self.n_outputs))\n",
    "            self.displayed_info = True\n",
    "            \n",
    "        with x.context:\n",
    "            regression_weights = tl.tucker_to_tensor(self.core.data(), [f.data() for f in self.factors])\n",
    "            temp = nd.dot(x.reshape((0, -1)), nd.reshape(regression_weights, (-1, self.n_outputs)))\n",
    "            return nd.broadcast_add(temp, self.bias.data())\n",
    "        \n",
    "    def penalty(self, order=2):\n",
    "        penalty = tl.norm(self.core.data(), order)\n",
    "        for f in self.factors:\n",
    "            penatly = penalty + tl.norm(f.data(), order)\n",
    "        return penalty\n",
    "    \n",
    "    @property\n",
    "    def regression_weights(self):\n",
    "        return tl.tucker_to_tensor(self.core.data(), [f.data() for f in self.factors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Convolutional Net with a low-rank tensor regression output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))            \n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # We keep a reference to the trl instance so we can refer to it later\n",
    "    trl = TRL([10, 3, 3, 10], num_outputs)\n",
    "    net.add(trl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL: initializing factors\n",
      "  ..done.\n",
      "TRL on input of shape=(50, 4, 4) with batch size=64.\n",
      "    TRL-rank=(10, 3, 3, 10) corresponding to ((50, 4, 4) [inputs], 10 [outputs])\n",
      "Epoch 0. Loss: 0.102192373141, Train_acc 0.972414712154, Test_acc 0.974820859873\n",
      "Epoch 1. Loss: 0.0664897566447, Train_acc 0.982292777186, Test_acc 0.983180732484\n",
      "Epoch 2. Loss: 0.0509061708328, Train_acc 0.986773720682, Test_acc 0.986464968153\n",
      "Epoch 3. Loss: 0.0417029955412, Train_acc 0.989438965885, Test_acc 0.988256369427\n",
      "Epoch 4. Loss: 0.0354006244203, Train_acc 0.990754930704, Test_acc 0.989052547771\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "smoothing_constant = .01\n",
    "rng = check_random_state(1) \n",
    "C = 0.001 # regularization\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_data.reset()\n",
    "    for i, batch in enumerate(train_data):\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            # We add a penalty, as defined in the TRL layer\n",
    "            loss = softmax_cross_entropy(output, label) + C*trl.penalty()\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0)) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "        \n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasn't that easy?? :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you want to, you can explore the learned regression weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4, 4, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trl.regression_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or even plot them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x123663470>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEICAYAAACnA7rCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhkJSy1BIahkKSa3/AEezkINBxWoIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1235734e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "W = tl.to_numpy(trl.regression_weights)\n",
    "ax.imshow(np.squeeze(W.mean(axis=0))[..., 0], cmap=plt.cm.OrRd, interpolation='nearest')\n",
    "ax.set_axis_off()\n",
    "ax.set_title('True regression weights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
